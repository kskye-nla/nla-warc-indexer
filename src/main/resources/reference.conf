# WARC configuration namespace:
warc {
  # Indexing configuration:
  index {
    # What to extract:
    extract {
      # Maximum length of payload on which to work:
      buffer_size: 20971520,
      # Which linked entities to extract:
      linked {
        resources: true,
        hosts: true,
        domains: true
      },
      
      # URLs to skip:
      #url_exclude: [robots.txt,.rss,panaccess-mime.types,.js,.cat,.css]
      url_exclude: [],
    
      # Restrict response codes:
      # works by matches starting with the characters, so "2" will match 2xx:
      response_include: ["2"],

      # Restrict protocols:
      protocol_include: [http,https]

      # Restrict record types:
      record_type_include: [
            response,
            revisit
      ],
      
      # Content to extract
      content {
        # Should we index the content body text?
        text: true,
        
        # Extract list of elements used in HTML:
        elements_used: true,
        
        # Extract potential PDF problems using Apache PDFBox Preflight:
        extractApachePreflightErrors: true,
        
        # Extract image features:
        extractImageFeatures: true,
        
        # Extract the first bytes of the file (for shingling):
        first_bytes {
          # Enabled?
          enabled: true,
          # Number of bytes to extract (>=4 to allow content_ffb to work):
          num_bytes: 32
        },
      },
    },
    
    # Parameters relating to format identification:   
    id {
      # Allow tools to infer format from the resource URI (file extension):
      useResourceURI: true,
      
      # DROID-specific config:
      droid {
        enabled: true,
        useBinarySignaturesOnly: false
      }
    },
    
    # Parameters to control Apache Tika behaviour
    tika {
    
      # Formats to avoid processing
      exclude_mime: [],
      # The parse timeout (for when Tika gets stuck):
      parse_timeout: 300000,
      
      # Maximum length of text to extract:
      max_text_length: 1024K
    },
    
    # Parameters to control the exclusion of results from the indexing process:
    exclusions {
      # Exclusion enabled?
      enabled: false,
      # Exclusion URI/SURT prefix file:
      file: "/path/to/exclude.txt",
      # Default check interval before reloading the exclusions file, in seconds:
      check_interval: 600
    },
      
  },
  
  # Solr configuration - CURRENTLY IGNORED BY THE NON-HADOOP INDEXER -
  solr {
    server: "http://localhost:8080/solr/discovery"
    batch_size: 50
    num_threads: 1
    # Check SOLR for duplicates during indexing:
    check_solr_for_duplicates: false
    # Is this a dummy-run? (i.e. should we NOT post to SOLR?)
    dummy_run: false
    # Use the hash+url as the ID for the documents
    "use_hash_url_id": true
  },
  
  # HTTP Proxy to use when talking to Solr (if any):
  http_proxy {
    #host: explorer.bl.uk,
    #port: 3127
  }
}
